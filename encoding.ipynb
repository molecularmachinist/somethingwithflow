{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eurastof/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis import align\n",
    "from MDAnalysis.analysis.distances import distance_array\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making coordinate data (b2ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "dirs = glob.glob(\"../binding_spots_project/gpcr_sampling/b2ar/b2ar_centered_aligned/*\")\n",
    "data = []\n",
    "ids = []\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    system = d.split(\"/\")[-1]\n",
    "    cosmos = mda.Universe(d + \"/step6.6_equilibration.gro\", glob.glob(f\"{d}/*xtc\"))\n",
    "    heavy = cosmos.select_atoms(\"protein and not type H\")\n",
    "    coords = []\n",
    "    for ts in cosmos.trajectory[0:-1:2]:\n",
    "        pos = heavy.positions.flatten()\n",
    "        coords.append(pos.reshape(1, pos.shape[0]))\n",
    "    coords = np.concatenate(coords, axis=0)\n",
    "    ids += [system for _ in range(coords.shape[0])]\n",
    "    data.append(coords)\n",
    "\n",
    "data = np.concatenate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cols = []\n",
    "for resid, i in enumerate(range(0, data.shape[1], 3)):\n",
    "    cols += [f\"{resid}x\", f\"{resid}y\", f\"{resid}z\"]\n",
    "\n",
    "D = pd.DataFrame(data, columns=cols)\n",
    "D[\"system\"] = ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.to_csv(\"./data/b2ar-heavy-atoms.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(BaseEstimator, TransformerMixin, nn.Module):\n",
    "\n",
    "    def __init__(self, in_shape=10, enc_shape=2, middle_shape=5, n_hidden=1, loss_fn=nn.L1Loss(), lr=1e-3):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr \n",
    "        self.n_hidden = n_hidden # number of hidden layers\n",
    "        self.in_shape = in_shape # input dimension\n",
    "        self.enc_shape = enc_shape # dimension of encoding\n",
    "        self.middle_shape = middle_shape # hidden layer dimensions\n",
    "        \n",
    "        encoder_layers = [nn.Linear(self.in_shape, self.middle_shape), nn.ReLU(), nn.Dropout(0.2)] # initialize encoder layer list\n",
    "        decoder_layers = [nn.Linear(self.enc_shape, self.middle_shape), nn.ReLU(), nn.Dropout(0.2)] # initialize decoder layer list\n",
    "\n",
    "        for i in range(n_hidden - 1): # Add layers to encoder and decoder according to n_hidden and middle shape\n",
    "            encoder_layers.append(nn.Linear(self.middle_shape, self.middle_shape))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            encoder_layers.append(nn.Dropout(0.2))\n",
    "            decoder_layers.append(nn.Linear(self.middle_shape, self.middle_shape))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            decoder_layers.append(nn.Dropout(0.2))\n",
    "            \n",
    "        encoder_layers.append(nn.Linear(self.middle_shape, self.enc_shape)) # Final encoder layer\n",
    "        decoder_layers.append(nn.Linear(self.middle_shape, self.in_shape)) # Final decoder layer\n",
    "        #decoder_layers.append(nn.Sigmoid()) # Sigmoid at end of deocder?\n",
    "\n",
    "        self.encode = nn.Sequential(*encoder_layers) # Make encoder\n",
    "        self.decode = nn.Sequential(*decoder_layers) # Make decoder\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None, n_epochs=20, batch_size=32, verbose=False):\n",
    "        self.training = True # Enables e.g. dropouts to work\n",
    "        X = torch.Tensor(X)\n",
    "        indices = [i for i in range(X.shape[0])]\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr) # Adam only atm\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "        \n",
    "            random.shuffle(indices) # random shuffle to get random batches for each epoch\n",
    "            batches = [i for i in range(0, len(indices), batch_size)]\n",
    "\n",
    "            for i in range(len(batches) - 1):\n",
    "\n",
    "                batch_X = X[indices[batches[i]:batches[i+1]]]\n",
    "                self.optimizer.zero_grad() # reset optimizer\n",
    "                \n",
    "                encoded = self.encode(batch_X)\n",
    "                decoded = self.decode(encoded)\n",
    "\n",
    "                loss = self.loss_fn(decoded, batch_X)\n",
    "                loss.backward() # Backpropagate\n",
    "                self.optimizer.step() # Apply changes\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'epoch {epoch} \\t Loss: {loss.item():.4g}')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        encoded = self.encode(torch.Tensor(X))\n",
    "        return encoded.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        decoded = self.decode(torch.Tensor(X))\n",
    "        return decoded.cpu().detach().numpy()\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        encoded = self.transform(X)\n",
    "        decoded = self.inverse_transform(encoded)\n",
    "        \n",
    "        return -self.loss_fn(torch.Tensor(X), torch.Tensor(decoded)) # Take negative to make GridSearchCV work properly\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "#device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"  # GridSearchCV having issues with GPUs\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of parameters for gridsearch\n",
    "\n",
    "param_grid = {\n",
    "    \"Autoencoder\":{\n",
    "        \"Autoencoder__middle_shape\": [512],\n",
    "        \"Autoencoder__enc_shape\": [2],\n",
    "        \"Autoencoder__n_hidden\": [1]\n",
    "    },\n",
    "    \"GMM\": {\n",
    "        \"n_components\": [4]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search parameters and return best model\n",
    "\n",
    "def best_pipeline(transformer, param_grid, X, cv=2):\n",
    "    \n",
    "    step_names = [\"Scaler\", \"Autoencoder\"]\n",
    "    params = {key: val for k, d in param_grid.items() for key, val in d.items() if k in step_names}\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (step_names[0], StandardScaler()),\n",
    "            (step_names[1], transformer),\n",
    "        ]   \n",
    "    )\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, param_grid=params, verbose=3, cv=cv)\n",
    "    gridsearch.fit(X)\n",
    "    \n",
    "    return gridsearch.best_estimator_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set\n",
    "\n",
    "encoded = best.transform(test_X)\n",
    "decoded = best.inverse_transform(torch.Tensor(encoded))\n",
    "\n",
    "# L1 loss (Ã…)\n",
    "\n",
    "error = np.mean(np.abs(decoded- test_X))\n",
    "print(f\"Reconstruction error: {error} Ã… \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input perturbation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IPA(X, model):\n",
    "        \n",
    "    \n",
    "    index = np.arange(0, X.shape[1], 3)\n",
    "    effects = []\n",
    "    \n",
    "    for i in index:\n",
    "        shuffled = X.copy()\n",
    "        shuffled = shuffle(shuffled)\n",
    "        rands = np.random.uniform(low=-5, high=5, size=(shuffled.shape[0],1))\n",
    "        rands = np.concatenate([rands, rands, rands], axis=1)\n",
    "        shuffled[:, i:i+3] = rands\n",
    "        encoded = model.transform(shuffled)\n",
    "        decoded = model.inverse_transform(encoded)\n",
    "        decoded = s.inverse_transform(decoded)\n",
    "        L1 = np.mean(np.abs(X - decoded))\n",
    "        effects += [L1, L1, L1]\n",
    "    \n",
    "    return effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosmos = mda.Universe(\"/wrk/eurastof/binding_spots_project/gpcr_sampling/b2ar/b2ar_centered_aligned/popc/step6.6_equilibration.gro\",\n",
    "                      \"/wrk/eurastof/binding_spots_project/gpcr_sampling/b2ar/b2ar_centered_aligned/popc/centered_aligned10.xtc\")\n",
    "with open(\"/wrk/eurastof/binding_spots_project/HFSP---Lipid-binding-states/calculations/b2ar_common.ndx\") as f:\n",
    "        lines = \"\".join(f.readlines())\n",
    "index = \" \".join(re.findall(r\"\\d+\", lines)[1:])\n",
    "common_ca = cosmos.select_atoms(f\"bynum {index}\")\n",
    "common_ca.write(\"common_ca.gro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc_vals = enc[:,1]\n",
    "n_bins = 10\n",
    "\n",
    "bins = np.linspace(np.min(enc_vals), np.max(enc_vals), n_bins)\n",
    "\n",
    "write_file = \"enc_dim_2.xtc\"\n",
    "\n",
    "\n",
    "with mda.Writer(write_file, common_ca.n_atoms) as W:\n",
    "    for i in range(bins.shape[0] - 1):\n",
    "        \n",
    "        in_bin = np.where((enc_vals > bins[i]) & (enc_vals < bins[i + 1]))[0]\n",
    "        \n",
    "        frames_in_bin = np.array(X[in_bin,:])\n",
    "        avg_frame_in_bin = np.mean(frames_in_bin, axis=0).reshape(1, frames_in_bin.shape[1])\n",
    "        distances_to_avg = np.mean(np.sqrt((frames_in_bin - avg_frame_in_bin)**2), axis=1)\n",
    "        closest = np.argmin(distances_to_avg)\n",
    "        closest_coords = frames_in_bin[closest,:].reshape(int(frames_in_bin.shape[1]/3), 3)\n",
    "        common_ca.positions = closest_coords\n",
    "        W.write(common_ca)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('sf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9367d5d3b3259b4d4bf01b62cc0f0e1ca449aad14498d84cee5292445dca8140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
